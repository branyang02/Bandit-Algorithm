# Bandit-Algorithm

This repo contains both multi-armed bandit algorithms and linear contextual bandit algorithms.

The following algorithms are implemented:
* Multi-Armed Bandit Algorithms:
  * Epsilon Greedy
  * Upper Confidence Bound (UCB1)
  * Thompson Sampling
  * Perturbed-History Exploration
* Contextual Bandit Algorithms:
  * Linear Epsilon Greedy
  * Linear Upper Confidence Bound (LinUCB)
  * Linear Thompson Sampling (LinTS)
